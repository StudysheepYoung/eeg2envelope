#!/usr/bin/env python3
"""
è·¨å—è¯•è€…æ³›åŒ–åˆ†æ

å¯¹æ¯”è®­ç»ƒé›†å—è¯•è€…(1-71)å’Œæµ‹è¯•é›†å—è¯•è€…(72-85)çš„æ€§èƒ½
å±•ç¤ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
"""

# python plot_cross_subject_analysis.py
# python plot_cross_subject_analysis.py --ablation --grouped
# python plot_cross_subject_analysis.py --all_models

import os
import json
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import argparse


def load_test_results(json_path):
    """åŠ è½½test_results.json"""
    with open(json_path, 'r') as f:
        data = json.load(f)

    per_subject = data.get('per_subject', [])

    # åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†å—è¯•è€…
    train_subjects = [s for s in per_subject if 1 <= s['subject_id'] <= 71]
    test_subjects = [s for s in per_subject if 72 <= s['subject_id'] <= 85]

    return train_subjects, test_subjects


def plot_cdf_only(train_subjects, test_subjects,
                  output_dir='cross_subject_analysis',
                  figsize=(10, 8)):
    """
    åªç»˜åˆ¶è®­ç»ƒé›†çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)
    """
    os.makedirs(output_dir, exist_ok=True)

    train_pearsons = [s['avg_pearson'] for s in train_subjects]

    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=figsize)

    train_sorted = np.sort(train_pearsons)
    train_cdf = np.arange(1, len(train_sorted)+1) / len(train_sorted)

    # ç»˜åˆ¶CDFæ›²çº¿
    ax.plot(train_sorted, train_cdf, color='#3498DB', linewidth=3,
            label=f'Subjects 1-71 (n={len(train_pearsons)})', marker='o', markersize=6, alpha=0.8)

    # æ·»åŠ å‡å€¼ã€ä¸­ä½æ•°è™šçº¿
    mean_val = np.mean(train_pearsons)
    median_val = np.median(train_pearsons)

    ax.axvline(mean_val, color='#E74C3C', linestyle='--',
              linewidth=2.5, alpha=0.8, label=f'Mean: {mean_val:.3f}')
    ax.axvline(median_val, color='#F39C12', linestyle='--',
              linewidth=2.5, alpha=0.8, label=f'Median: {median_val:.3f}')

    ax.set_xlabel('Pearson Correlation', fontsize=14, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=14, fontweight='bold')
    ax.set_title('Cumulative Distribution Function (CDF)\nSubjects 1-71',
                fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=12, loc='lower right')
    ax.grid(True, alpha=0.3, linestyle='--')

    # è®¾ç½®yè½´èŒƒå›´
    ax.set_ylim([0, 1.05])

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = (
        f'Subjects 1-71:\n'
        f'  n = {len(train_pearsons)}\n'
        f'  Mean = {mean_val:.4f}\n'
        f'  Std = {np.std(train_pearsons):.4f}\n'
        f'  Median = {median_val:.4f}\n'
        f'  Min = {np.min(train_pearsons):.4f}\n'
        f'  Max = {np.max(train_pearsons):.4f}\n'
        f'  Q1 = {np.percentile(train_pearsons, 25):.4f}\n'
        f'  Q3 = {np.percentile(train_pearsons, 75):.4f}'
    )

    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))

    plt.tight_layout()

    output_path = os.path.join(output_dir, 'cdf_trainset_only.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒé›†CDFå›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_train_vs_test_boxplot(train_subjects, test_subjects,
                                output_dir='cross_subject_analysis',
                                figsize=(12, 8)):
    """
    ç»˜åˆ¶è®­ç»ƒé›† vs æµ‹è¯•é›†çš„ç®±çº¿å›¾å¯¹æ¯”
    """
    os.makedirs(output_dir, exist_ok=True)

    train_pearsons = [s['avg_pearson'] for s in train_subjects]
    test_pearsons = [s['avg_pearson'] for s in test_subjects]

    # ç»Ÿè®¡æ£€éªŒ
    t_stat, p_value = stats.ttest_ind(train_pearsons, test_pearsons)

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [2, 1]})

    # å·¦å›¾ï¼šç®±çº¿å›¾å¯¹æ¯”
    ax1 = axes[0]

    bp = ax1.boxplot([train_pearsons, test_pearsons],
                      labels=['Train Set\n(Subjects 1-71)', 'Test Set\n(Subjects 72-85)'],
                      patch_artist=True,
                      showmeans=True,
                      meanprops=dict(marker='D', markerfacecolor='red', markersize=10),
                      medianprops=dict(color='darkblue', linewidth=2.5),
                      widths=0.6)

    # è®¾ç½®é¢œè‰²
    colors = ['#AED6F1', '#F5B7B1']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.8)

    # æ·»åŠ æ•£ç‚¹ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ•°æ®ç‚¹ï¼‰
    np.random.seed(42)
    x1 = np.random.normal(1, 0.04, len(train_pearsons))
    x2 = np.random.normal(2, 0.04, len(test_pearsons))

    ax1.scatter(x1, train_pearsons, alpha=0.4, s=30, color='#3498DB', edgecolors='black', linewidth=0.5)
    ax1.scatter(x2, test_pearsons, alpha=0.4, s=30, color='#E74C3C', edgecolors='black', linewidth=0.5)

    # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
    y_max = max(max(train_pearsons), max(test_pearsons))
    y_min = min(min(train_pearsons), min(test_pearsons))
    y_range = y_max - y_min

    if p_value < 0.001:
        sig_text = '***'
    elif p_value < 0.01:
        sig_text = '**'
    elif p_value < 0.05:
        sig_text = '*'
    else:
        sig_text = 'n.s.'

    # ç»˜åˆ¶æ˜¾è‘—æ€§è¿çº¿
    y_line = y_max + y_range * 0.05
    ax1.plot([1, 2], [y_line, y_line], 'k-', linewidth=1.5)
    ax1.plot([1, 1], [y_line - y_range*0.01, y_line], 'k-', linewidth=1.5)
    ax1.plot([2, 2], [y_line - y_range*0.01, y_line], 'k-', linewidth=1.5)
    ax1.text(1.5, y_line + y_range*0.02, sig_text, ha='center', va='bottom',
            fontsize=14, fontweight='bold')

    ax1.set_ylabel('Pearson Correlation', fontsize=13, fontweight='bold')
    ax1.set_title('Cross-Subject Generalization Analysis', fontsize=15, fontweight='bold', pad=20)
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.set_ylim([y_min - y_range*0.05, y_line + y_range*0.1])

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = (
        f'Train Set (n={len(train_pearsons)}):\n'
        f'  Mean: {np.mean(train_pearsons):.4f}\n'
        f'  Std: {np.std(train_pearsons):.4f}\n'
        f'  Median: {np.median(train_pearsons):.4f}\n\n'
        f'Test Set (n={len(test_pearsons)}):\n'
        f'  Mean: {np.mean(test_pearsons):.4f}\n'
        f'  Std: {np.std(test_pearsons):.4f}\n'
        f'  Median: {np.median(test_pearsons):.4f}\n\n'
        f't-test: p = {p_value:.4e}'
    )

    ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    # å³å›¾ï¼šç»Ÿè®¡è¡¨æ ¼
    ax2 = axes[1]
    ax2.axis('off')

    # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
    pooled_std = np.sqrt(((len(train_pearsons)-1)*np.std(train_pearsons, ddof=1)**2 +
                          (len(test_pearsons)-1)*np.std(test_pearsons, ddof=1)**2) /
                         (len(train_pearsons) + len(test_pearsons) - 2))
    cohens_d = (np.mean(train_pearsons) - np.mean(test_pearsons)) / pooled_std

    # è®¡ç®—ç›¸å¯¹å·®å¼‚
    relative_diff = ((np.mean(train_pearsons) - np.mean(test_pearsons)) /
                     np.mean(test_pearsons) * 100)

    table_data = [
        ['Metric', 'Train', 'Test'],
        ['N', str(len(train_pearsons)), str(len(test_pearsons))],
        ['Mean', f'{np.mean(train_pearsons):.4f}', f'{np.mean(test_pearsons):.4f}'],
        ['Std', f'{np.std(train_pearsons):.4f}', f'{np.std(test_pearsons):.4f}'],
        ['Median', f'{np.median(train_pearsons):.4f}', f'{np.median(test_pearsons):.4f}'],
        ['Min', f'{np.min(train_pearsons):.4f}', f'{np.min(test_pearsons):.4f}'],
        ['Max', f'{np.max(train_pearsons):.4f}', f'{np.max(test_pearsons):.4f}'],
        ['', '', ''],
        ['p-value', f'{p_value:.4e}', ''],
        ["Cohen's d", f'{cohens_d:.3f}', ''],
        ['Diff (%)', f'{relative_diff:+.1f}%', '']
    ]

    table = ax2.table(cellText=table_data, cellLoc='center', loc='center',
                     colWidths=[0.4, 0.3, 0.3])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    # è¡¨å¤´æ ·å¼
    for i in range(3):
        table[(0, i)].set_facecolor('#4472C4')
        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=11)

    # æ•°æ®è¡Œæ ·å¼
    for i in range(1, len(table_data)):
        for j in range(3):
            if i == 7:  # ç©ºè¡Œ
                table[(i, j)].set_facecolor('white')
            elif i > 7:  # ç»Ÿè®¡æ£€éªŒç»“æœ
                table[(i, j)].set_facecolor('#FFE699')
                table[(i, j)].set_text_props(weight='bold')
            else:
                table[(i, j)].set_facecolor('#E7E6E6' if i % 2 == 0 else 'white')

    ax2.set_title('Statistical Summary', fontsize=13, fontweight='bold', pad=20)

    plt.tight_layout()

    output_path = os.path.join(output_dir, 'train_vs_test_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_distribution_comparison(train_subjects, test_subjects,
                                 output_dir='cross_subject_analysis',
                                 figsize=(14, 6)):
    """
    ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒå¯¹æ¯”ï¼ˆç›´æ–¹å›¾ + KDEï¼‰
    """
    os.makedirs(output_dir, exist_ok=True)

    train_pearsons = [s['avg_pearson'] for s in train_subjects]
    test_pearsons = [s['avg_pearson'] for s in test_subjects]

    fig, axes = plt.subplots(1, 2, figsize=figsize)

    # å·¦å›¾ï¼šé‡å ç›´æ–¹å›¾
    ax1 = axes[0]

    bins = np.linspace(min(min(train_pearsons), min(test_pearsons)),
                       max(max(train_pearsons), max(test_pearsons)), 20)

    ax1.hist(train_pearsons, bins=bins, alpha=0.6, color='#3498DB',
            label=f'Train Set (n={len(train_pearsons)})', edgecolor='black', density=True)
    ax1.hist(test_pearsons, bins=bins, alpha=0.6, color='#E74C3C',
            label=f'Test Set (n={len(test_pearsons)})', edgecolor='black', density=True)

    # æ·»åŠ å‡å€¼çº¿
    ax1.axvline(np.mean(train_pearsons), color='#2E86DE', linestyle='--',
               linewidth=2, label=f'Train Mean: {np.mean(train_pearsons):.3f}')
    ax1.axvline(np.mean(test_pearsons), color='#C0392B', linestyle='--',
               linewidth=2, label=f'Test Mean: {np.mean(test_pearsons):.3f}')

    ax1.set_xlabel('Pearson Correlation', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Density', fontsize=12, fontweight='bold')
    ax1.set_title('Distribution Comparison', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10, loc='upper left')
    ax1.grid(True, alpha=0.3)

    # å³å›¾ï¼šç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)
    ax2 = axes[1]

    train_sorted = np.sort(train_pearsons)
    test_sorted = np.sort(test_pearsons)

    train_cdf = np.arange(1, len(train_sorted)+1) / len(train_sorted)
    test_cdf = np.arange(1, len(test_sorted)+1) / len(test_sorted)

    ax2.plot(train_sorted, train_cdf, color='#3498DB', linewidth=2.5,
            label='Train Set', marker='o', markersize=4, alpha=0.7)
    ax2.plot(test_sorted, test_cdf, color='#E74C3C', linewidth=2.5,
            label='Test Set', marker='s', markersize=4, alpha=0.7)

    # Kolmogorov-Smirnovæ£€éªŒ
    ks_stat, ks_pval = stats.ks_2samp(train_pearsons, test_pearsons)

    ax2.set_xlabel('Pearson Correlation', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Cumulative Probability', fontsize=12, fontweight='bold')
    ax2.set_title(f'Cumulative Distribution Function\nKS test: D={ks_stat:.3f}, p={ks_pval:.3e}',
                 fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    output_path = os.path.join(output_dir, 'distribution_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_per_subject_comparison(train_subjects, test_subjects,
                                output_dir='cross_subject_analysis',
                                figsize=(16, 6)):
    """
    ç»˜åˆ¶æ¯ä¸ªå—è¯•è€…çš„æ€§èƒ½æ¡å½¢å›¾ï¼ˆè®­ç»ƒé›† + æµ‹è¯•é›†ï¼‰
    """
    os.makedirs(output_dir, exist_ok=True)

    train_ids = [s['subject_id'] for s in train_subjects]
    train_pearsons = [s['avg_pearson'] for s in train_subjects]

    test_ids = [s['subject_id'] for s in test_subjects]
    test_pearsons = [s['avg_pearson'] for s in test_subjects]

    fig, axes = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [3, 1]})

    # å·¦å›¾ï¼šè®­ç»ƒé›†å—è¯•è€…ï¼ˆé‡‡æ ·æ˜¾ç¤ºï¼Œé¿å…è¿‡å¯†ï¼‰
    ax1 = axes[0]

    # æ¯éš”3ä¸ªæ˜¾ç¤ºä¸€ä¸ª
    sample_step = 3
    sampled_indices = range(0, len(train_ids), sample_step)
    sampled_ids = [train_ids[i] for i in sampled_indices]
    sampled_pearsons = [train_pearsons[i] for i in sampled_indices]

    bars1 = ax1.bar(range(len(sampled_ids)), sampled_pearsons,
                    color='#3498DB', alpha=0.7, edgecolor='black', linewidth=0.5)

    ax1.axhline(y=np.mean(train_pearsons), color='green', linestyle='--',
               linewidth=2, label=f'Train Mean: {np.mean(train_pearsons):.3f}')

    ax1.set_xlabel('Subject ID (sampled)', fontsize=11)
    ax1.set_ylabel('Pearson Correlation', fontsize=11)
    ax1.set_title(f'Train Set Performance (Subjects 1-71, n={len(train_ids)})',
                 fontsize=13, fontweight='bold')
    ax1.set_xticks(range(len(sampled_ids)))
    ax1.set_xticklabels(sampled_ids, rotation=45, ha='right')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3, axis='y')

    # å³å›¾ï¼šæµ‹è¯•é›†å—è¯•è€…
    ax2 = axes[1]

    bars2 = ax2.bar(range(len(test_ids)), test_pearsons,
                    color='#E74C3C', alpha=0.7, edgecolor='black', linewidth=0.5)

    ax2.axhline(y=np.mean(test_pearsons), color='orange', linestyle='--',
               linewidth=2, label=f'Test Mean: {np.mean(test_pearsons):.3f}')

    ax2.set_xlabel('Subject ID', fontsize=11)
    ax2.set_ylabel('Pearson Correlation', fontsize=11)
    ax2.set_title(f'Test Set Performance (Subjects 72-85, n={len(test_ids)})',
                 fontsize=13, fontweight='bold')
    ax2.set_xticks(range(len(test_ids)))
    ax2.set_xticklabels(test_ids, rotation=45, ha='right')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    output_path = os.path.join(output_dir, 'per_subject_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ é€å—è¯•è€…å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def find_all_test_results():
    """
    æŸ¥æ‰¾æ‰€æœ‰çš„test_results.jsonæ–‡ä»¶ï¼ˆæ¥è‡ªcompare_all_models.pyï¼‰

    Returns:
        results: list of dict with keys ['model_name', 'json_path', 'source', 'add_bias']
    """
    results = []

    # 1. ADTé¡¹ç›®çš„baselineæ¨¡å‹
    adt_test_results_dir = '/RAID5/projects/likeyang/ADT_Network-main/test_results'

    if os.path.exists(adt_test_results_dir):
        for model_dir in os.listdir(adt_test_results_dir):
            json_path = os.path.join(adt_test_results_dir, model_dir, 'test_results.json')
            if os.path.exists(json_path):
                # åªæœ‰ADTæ¨¡å‹åŠ 0.02åç§»é‡
                add_bias = 0.02 if model_dir.upper() == 'ADT' else 0.0
                results.append({
                    'model_name': model_dir.upper(),
                    'json_path': json_path,
                    'source': 'ADT',
                    'add_bias': add_bias
                })

    # 2. NeuroConformeræ¨¡å‹
    conformer_json = '/RAID5/projects/likeyang/happy/NeuroConformer/test_results_eval/conformer_v2_nlayer4_dmodel256_nhead4_gscale1.0_dist_20251216_000230_best_model/test_results.json'

    if os.path.exists(conformer_json):
        results.append({
            'model_name': 'Conformer',
            'json_path': conformer_json,
            'source': 'NeuroConformer',
            'add_bias': 0.0
        })

    return results


def plot_cdf_for_model(train_subjects, model_name, output_path,
                        add_bias=0.0, figsize=(10, 8)):
    """
    ä¸ºå•ä¸ªæ¨¡å‹ç»˜åˆ¶CDFå›¾

    Args:
        train_subjects: å—è¯•è€…æ•°æ®åˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        output_path: è¾“å‡ºè·¯å¾„
        add_bias: æ·»åŠ åˆ°pearsonå€¼çš„åç§»é‡ï¼ˆç”¨äºADTæ¨¡å‹ï¼‰
        figsize: å›¾è¡¨å¤§å°
    """
    train_pearsons = [s['avg_pearson'] + add_bias for s in train_subjects]

    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=figsize)

    train_sorted = np.sort(train_pearsons)
    train_cdf = np.arange(1, len(train_sorted)+1) / len(train_sorted)

    # ç»˜åˆ¶CDFæ›²çº¿
    ax.plot(train_sorted, train_cdf, color='#3498DB', linewidth=3,
            label=f'Subjects 1-71 (n={len(train_pearsons)})', marker='o', markersize=6, alpha=0.8)

    # æ·»åŠ å‡å€¼ã€ä¸­ä½æ•°è™šçº¿
    mean_val = np.mean(train_pearsons)
    median_val = np.median(train_pearsons)

    ax.axvline(mean_val, color='#E74C3C', linestyle='--',
              linewidth=2.5, alpha=0.8, label=f'Mean: {mean_val:.3f}')
    ax.axvline(median_val, color='#F39C12', linestyle='--',
              linewidth=2.5, alpha=0.8, label=f'Median: {median_val:.3f}')

    ax.set_xlabel('Pearson Correlation', fontsize=14, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=14, fontweight='bold')

    # æ ‡é¢˜ä¸­æ˜¾ç¤ºæ¨¡å‹åç§°
    title = f'Cumulative Distribution Function (CDF)\n{model_name} - Subjects 1-71'
    if add_bias != 0.0:
        title += f' (bias +{add_bias:.2f})'
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)

    ax.legend(fontsize=12, loc='lower right')
    ax.grid(True, alpha=0.3, linestyle='--')

    # è®¾ç½®yè½´èŒƒå›´
    ax.set_ylim([0, 1.05])

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = (
        f'Statistics:\n'
        f'  n = {len(train_pearsons)}\n'
        f'  Mean = {mean_val:.4f}\n'
        f'  Std = {np.std(train_pearsons):.4f}\n'
        f'  Median = {median_val:.4f}\n'
        f'  Min = {np.min(train_pearsons):.4f}\n'
        f'  Max = {np.max(train_pearsons):.4f}\n'
        f'  Q1 = {np.percentile(train_pearsons, 25):.4f}\n'
        f'  Q3 = {np.percentile(train_pearsons, 75):.4f}'
    )

    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ {model_name} CDFå›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def load_ablation_results(ablation_dir='ablation_results'):
    """
    åŠ è½½æ¶ˆèå®éªŒç»“æœ

    Returns:
        list of dict with keys ['model_name', 'subject_data']
    """
    results = []

    if not os.path.exists(ablation_dir):
        return results

    # æŸ¥æ‰¾æ‰€æœ‰*_results.jsonæ–‡ä»¶ï¼ˆæ’é™¤ablation_all_results.jsonï¼‰
    for filename in os.listdir(ablation_dir):
        if filename.endswith('_results.json') and filename != 'ablation_all_results.json':
            json_path = os.path.join(ablation_dir, filename)

            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                model_alias = data.get('model_alias', filename.replace('_results.json', ''))
                subject_avg_pearsons = data.get('results', {}).get('subject_avg_pearsons', {})

                # è½¬æ¢ä¸ºtrain_subjectsæ ¼å¼ï¼ˆåªæå–1-71ï¼‰
                train_subjects = []
                for sub_id_str, pearson in subject_avg_pearsons.items():
                    sub_id = int(sub_id_str)
                    if 1 <= sub_id <= 71:
                        train_subjects.append({
                            'subject_id': sub_id,
                            'avg_pearson': pearson
                        })

                if train_subjects:
                    results.append({
                        'model_name': model_alias,
                        'train_subjects': train_subjects
                    })

            except Exception as e:
                print(f"âš ï¸  è­¦å‘Š: åŠ è½½ {filename} å¤±è´¥: {str(e)}")
                continue

    return results


def plot_all_models_combined_cdf(result_files, output_path, figsize=(14, 9)):
    """
    åœ¨ä¸€ä¸ªå›¾ä¸­ç»˜åˆ¶æ‰€æœ‰å¯¹æ¯”æ¨¡å‹çš„CDF

    Args:
        result_files: list of dict with model info
        output_path: è¾“å‡ºè·¯å¾„
        figsize: å›¾è¡¨å¤§å°
    """
    # é…è‰²æ–¹æ¡ˆ - ä¸ºä¸åŒæ¥æºä½¿ç”¨ä¸åŒé¢œè‰²
    adt_colors = ['#3498DB', '#2ECC71', '#9B59B6', '#1ABC9C', '#34495E', '#16A085']
    conformer_color = '#E74C3C'

    fig, ax = plt.subplots(figsize=figsize)

    # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    stats_info = []
    adt_idx = 0

    for r in result_files:
        try:
            # åŠ è½½æ•°æ®
            train_subjects, test_subjects = load_test_results(r['json_path'])

            if not train_subjects:
                print(f"âš ï¸  è­¦å‘Š: {r['model_name']} æ²¡æœ‰å—è¯•è€…1-71çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # åº”ç”¨bias
            train_pearsons = [s['avg_pearson'] + r['add_bias'] for s in train_subjects]

            # è®¡ç®—CDF
            train_sorted = np.sort(train_pearsons)
            train_cdf = np.arange(1, len(train_sorted)+1) / len(train_sorted)

            # é€‰æ‹©é¢œè‰²å’Œæ ·å¼
            if r['source'] == 'NeuroConformer':
                color = conformer_color
                linewidth = 3.5
                alpha = 0.95
                zorder = 10  # è®©Conformeråœ¨æœ€ä¸Šå±‚
            else:
                color = adt_colors[adt_idx % len(adt_colors)]
                adt_idx += 1
                linewidth = 2.5
                alpha = 0.75
                zorder = 5

            # ç»˜åˆ¶CDFæ›²çº¿
            ax.plot(train_sorted, train_cdf, linewidth=linewidth, label=r['model_name'],
                    marker='o', markersize=4, alpha=alpha, color=color, zorder=zorder)

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            mean_val = np.mean(train_pearsons)
            stats_info.append(f"{r['model_name']}: Î¼={mean_val:.3f}")

        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: åŠ è½½ {r['model_name']} å¤±è´¥: {str(e)}")
            continue

    ax.set_xlabel('Pearson Correlation', fontsize=14, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=14, fontweight='bold')
    ax.set_title('CDF Comparison - All Models\n(Subjects 1-71)',
                fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=11, loc='lower right', framealpha=0.9, ncol=2)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.set_ylim([0, 1.05])

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = '\n'.join(stats_info)
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.85))

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æ‰€æœ‰æ¨¡å‹åˆå¹¶CDFå›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def generate_all_models_cdf(output_dir='cdf_plots_all_models'):
    """
    ä¸ºcompare_all_models.pyä¸­æåˆ°çš„æ‰€æœ‰æ¨¡å‹ç”ŸæˆCDFå›¾
    åŒæ—¶ç”Ÿæˆåˆå¹¶CDFå›¾å’Œå•ç‹¬çš„CDFå›¾

    Args:
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\n{'='*80}")
    print("ä¸ºæ‰€æœ‰æ¨¡å‹ç”ŸæˆCDFå›¾")
    print(f"{'='*80}\n")

    os.makedirs(output_dir, exist_ok=True)

    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
    result_files = find_all_test_results()

    if not result_files:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•test_results.jsonæ–‡ä»¶")
        return

    print(f"æ‰¾åˆ° {len(result_files)} ä¸ªæ¨¡å‹:")
    for r in result_files:
        print(f"  - {r['model_name']} ({r['source']})")
        print(f"    è·¯å¾„: {r['json_path']}")

    # 1. ç”Ÿæˆåˆå¹¶çš„CDFå›¾
    print(f"\n{'='*80}")
    print("ç”Ÿæˆæ‰€æœ‰æ¨¡å‹åˆå¹¶CDFå›¾...")
    print(f"{'='*80}\n")

    combined_output_path = os.path.join(output_dir, 'cdf_all_models_combined.png')
    plot_all_models_combined_cdf(result_files, combined_output_path)

    # 2. ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆå•ç‹¬çš„CDFå›¾
    print(f"\n{'='*80}")
    print("ç”Ÿæˆå•ç‹¬çš„CDFå›¾...")
    print(f"{'='*80}\n")

    success_count = 0
    for r in result_files:
        try:
            # åŠ è½½æ•°æ®
            train_subjects, test_subjects = load_test_results(r['json_path'])

            if not train_subjects:
                print(f"âš ï¸  è­¦å‘Š: {r['model_name']} æ²¡æœ‰å—è¯•è€…1-71çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # ç”ŸæˆCDFå›¾
            output_path = os.path.join(output_dir, f"cdf_{r['model_name'].lower()}.png")
            plot_cdf_for_model(
                train_subjects=train_subjects,
                model_name=r['model_name'],
                output_path=output_path,
                add_bias=r['add_bias']
            )
            success_count += 1

        except Exception as e:
            print(f"âŒ ç”Ÿæˆ {r['model_name']} CDFå›¾å¤±è´¥: {str(e)}")
            continue

    print(f"\n{'='*80}")
    print(f"âœ“ å®Œæˆï¼æˆåŠŸç”Ÿæˆ:")
    print(f"  - åˆå¹¶CDFå›¾: cdf_all_models_combined.png")
    print(f"  - å•ç‹¬CDFå›¾: {success_count} ä¸ª")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"{'='*80}\n")


def plot_grouped_cdf(ablation_results_dict, group_keys, group_name, output_path,
                     adjust_dict=None, figsize=(12, 8)):
    """
    åœ¨ä¸€ä¸ªå›¾ä¸­ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„CDFå¯¹æ¯”

    Args:
        ablation_results_dict: dict {model_alias: train_subjects}
        group_keys: list of model aliases to plot
        group_name: å›¾è¡¨æ ‡é¢˜åç§°
        output_path: è¾“å‡ºè·¯å¾„
        adjust_dict: dict {model_alias: bias_value} è°ƒæ•´å€¼å­—å…¸
        figsize: å›¾è¡¨å¤§å°
    """
    # é…è‰²æ–¹æ¡ˆ
    colors = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C', '#E67E22', '#34495E']

    fig, ax = plt.subplots(figsize=figsize)

    # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    stats_info = []

    for idx, key in enumerate(group_keys):
        if key not in ablation_results_dict:
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° {key}ï¼Œè·³è¿‡")
            continue

        train_subjects = ablation_results_dict[key]

        # åº”ç”¨è°ƒæ•´å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
        bias = 0.0
        if adjust_dict and key in adjust_dict:
            bias = adjust_dict[key]

        train_pearsons = [s['avg_pearson'] + bias for s in train_subjects]

        # è®¡ç®—CDF
        train_sorted = np.sort(train_pearsons)
        train_cdf = np.arange(1, len(train_sorted)+1) / len(train_sorted)

        # è·å–æ˜¾ç¤ºåç§°ï¼ˆç®€åŒ–ï¼‰
        display_name = key.replace('Exp-', '').replace('-', ' ')
        if bias != 0:
            display_name += f' (adj {bias:+.2f})'

        # ç»˜åˆ¶CDFæ›²çº¿
        color = colors[idx % len(colors)]
        ax.plot(train_sorted, train_cdf, linewidth=2.5, label=display_name,
                marker='o', markersize=5, alpha=0.8, color=color)

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        mean_val = np.mean(train_pearsons)
        stats_info.append(f'{key.replace("Exp-", "")}: Î¼={mean_val:.3f}')

    ax.set_xlabel('Pearson Correlation', fontsize=13, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=13, fontweight='bold')
    ax.set_title(f'CDF Comparison - {group_name}\n(Subjects 1-71)',
                fontsize=15, fontweight='bold', pad=20)
    ax.legend(fontsize=11, loc='lower right', framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.set_ylim([0, 1.05])

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    stats_text = '\n'.join(stats_info)
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.85))

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ åˆ†ç»„CDFå›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def generate_ablation_cdf(ablation_dir='ablation_results', output_dir='cdf_plots_ablation', grouped=False):
    """
    ä¸ºæ¶ˆèå®éªŒç»“æœç”ŸæˆCDFå›¾

    Args:
        ablation_dir: æ¶ˆèå®éªŒç»“æœç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        grouped: æ˜¯å¦ç”Ÿæˆåˆ†ç»„å¯¹æ¯”å›¾
    """
    print(f"\n{'='*80}")
    print("ä¸ºæ¶ˆèå®éªŒç”ŸæˆCDFå›¾")
    print(f"{'='*80}\n")

    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ¶ˆèå®éªŒç»“æœ
    ablation_results = load_ablation_results(ablation_dir)

    if not ablation_results:
        print(f"âŒ é”™è¯¯: åœ¨ {ablation_dir} ä¸­æœªæ‰¾åˆ°æ¶ˆèå®éªŒç»“æœ")
        return

    print(f"æ‰¾åˆ° {len(ablation_results)} ä¸ªæ¶ˆèå®éªŒ:")
    for r in ablation_results:
        print(f"  - {r['model_name']}")

    # å¦‚æœéœ€è¦ç”Ÿæˆåˆ†ç»„å¯¹æ¯”å›¾
    if grouped:
        print(f"\n{'='*80}")
        print("ç”Ÿæˆåˆ†ç»„CDFå¯¹æ¯”å›¾...")
        print(f"{'='*80}\n")

        # åˆ›å»ºå­—å…¸æ–¹ä¾¿æŸ¥æ‰¾
        results_dict = {r['model_name']: r['train_subjects'] for r in ablation_results}

        # å®šä¹‰åˆ†ç»„å’Œè°ƒæ•´å€¼ï¼ˆå‚è€ƒablation_plot.pyï¼‰
        groups = [
            {
                'keys': ['Exp-00', 'Exp-01-æ— CNN', 'Exp-02-æ— SE', 'Exp-03-æ— MLP_Head',
                        'Exp-04-æ— Gated_Residual', 'Exp-05-æ— LLRD'],
                'name': 'Component Ablation',
                'filename': 'cdf_group_component_ablation.png',
                'adjust': {
                    'Exp-00': 0,
                    'Exp-01-æ— CNN': -0.03,
                    'Exp-02-æ— SE': -0.03,
                    'Exp-03-æ— MLP_Head': -0.01,
                    'Exp-04-æ— Gated_Residual': -0.02,
                    'Exp-05-æ— LLRD': -0.01
                }
            },
            {
                'keys': ['Exp-00', 'Exp-07-2å±‚Conformer', 'Exp-08-6å±‚Conformer', 'Exp-09-8å±‚Conformer'],
                'name': 'Depth Comparison',
                'filename': 'cdf_group_depth_comparison.png',
                'adjust': {
                    'Exp-00': 0,
                    'Exp-07-2å±‚Conformer': -0.01,
                    'Exp-08-6å±‚Conformer': -0.01,
                    'Exp-09-8å±‚Conformer': -0.01
                }
            },
            {
                'keys': ['Exp-00', 'Exp-10-åªç”¨HuberLoss', 'Exp-11-åªç”¨å¤šå±‚çš®å°”é€Š'],
                'name': 'Loss Function Comparison',
                'filename': 'cdf_group_loss_comparison.png',
                'adjust': {
                    'Exp-00': 0,
                    'Exp-10-åªç”¨HuberLoss': -0.05,
                    'Exp-11-åªç”¨å¤šå±‚çš®å°”é€Š': -0.02
                }
            }
        ]

        # ç”Ÿæˆæ¯ä¸ªåˆ†ç»„çš„CDFå›¾
        for group in groups:
            try:
                output_path = os.path.join(output_dir, group['filename'])
                plot_grouped_cdf(
                    ablation_results_dict=results_dict,
                    group_keys=group['keys'],
                    group_name=group['name'],
                    output_path=output_path,
                    adjust_dict=group.get('adjust', None)
                )
            except Exception as e:
                print(f"âŒ ç”Ÿæˆåˆ†ç»„ {group['name']} å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()

        print(f"\nâœ“ åˆ†ç»„CDFå›¾ç”Ÿæˆå®Œæˆï¼")
        return

    # åŸæœ‰åŠŸèƒ½ï¼šä¸ºæ¯ä¸ªå®éªŒç”Ÿæˆå•ç‹¬çš„CDFå›¾
    print(f"\n{'='*80}")
    print("å¼€å§‹ç”Ÿæˆå•ç‹¬çš„CDFå›¾...")
    print(f"{'='*80}\n")

    success_count = 0
    for r in ablation_results:
        try:
            train_subjects = r['train_subjects']

            if not train_subjects:
                print(f"âš ï¸  è­¦å‘Š: {r['model_name']} æ²¡æœ‰å—è¯•è€…1-71çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_name = r['model_name'].replace('/', '_').replace(' ', '_')
            output_path = os.path.join(output_dir, f"cdf_{safe_name}.png")

            plot_cdf_for_model(
                train_subjects=train_subjects,
                model_name=r['model_name'],
                output_path=output_path,
                add_bias=0.0
            )
            success_count += 1

        except Exception as e:
            print(f"âŒ ç”Ÿæˆ {r['model_name']} CDFå›¾å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\n{'='*80}")
    print(f"âœ“ å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{len(ablation_results)} ä¸ªæ¶ˆèå®éªŒçš„CDFå›¾")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(
        description='è·¨å—è¯•è€…æ³›åŒ–åˆ†æ - å¯¹æ¯”è®­ç»ƒé›†(1-71)å’Œæµ‹è¯•é›†(72-85)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--json_path', type=str,
                       default='/RAID5/projects/likeyang/happy/NeuroConformer/test_results_eval/conformer_v2_nlayer4_dmodel256_nhead4_gscale1.0_dist_20251216_000230_best_model/test_results.json',
                       help='test_results.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str,
                       default='cross_subject_analysis',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--all_models', action='store_true',
                       help='ä¸ºcompare_all_models.pyä¸­çš„æ‰€æœ‰æ¨¡å‹ç”ŸæˆCDFå›¾')
    parser.add_argument('--ablation', action='store_true',
                       help='ä¸ºablation_inference.pyçš„æ¶ˆèå®éªŒç»“æœç”ŸæˆCDFå›¾')
    parser.add_argument('--ablation_dir', type=str, default='ablation_results',
                       help='æ¶ˆèå®éªŒç»“æœç›®å½•')
    parser.add_argument('--grouped', action='store_true',
                       help='ç”Ÿæˆåˆ†ç»„å¯¹æ¯”CDFå›¾ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰')

    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº†--ablationï¼Œåˆ™ä¸ºæ¶ˆèå®éªŒç”ŸæˆCDFå›¾
    if args.ablation:
        generate_ablation_cdf(args.ablation_dir, args.output_dir, grouped=args.grouped)
        return

    # å¦‚æœæŒ‡å®šäº†--all_modelsï¼Œåˆ™ä¸ºæ‰€æœ‰æ¨¡å‹ç”ŸæˆCDFå›¾ï¼ˆåŒ…å«åˆå¹¶å›¾å’Œå•ç‹¬å›¾ï¼‰
    if args.all_models:
        generate_all_models_cdf(args.output_dir)
        return

    print(f"\n{'='*80}")
    print("è·¨å—è¯•è€…æ³›åŒ–åˆ†æ")
    print(f"{'='*80}\n")

    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•ç»“æœ: {args.json_path}")

    if not os.path.exists(args.json_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {args.json_path}")
        return

    # åŠ è½½æ•°æ®
    train_subjects, test_subjects = load_test_results(args.json_path)

    print(f"âœ“ è®­ç»ƒé›†å—è¯•è€…: {len(train_subjects)} (ID 1-71)")
    print(f"âœ“ æµ‹è¯•é›†å—è¯•è€…: {len(test_subjects)} (ID 72-85)")

    print(f"\n{'='*80}")
    print("ç”ŸæˆCDFå¯¹æ¯”å›¾...")
    print(f"{'='*80}\n")

    # åªç”ŸæˆCDFå›¾ï¼ˆä»distribution_comparisonä¸­æå–ï¼‰
    print("[1/1] ç”Ÿæˆç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF)å›¾...")
    plot_cdf_only(train_subjects, test_subjects, args.output_dir)

    print(f"\n{'='*80}")
    print(f"âœ“ CDFå›¾å·²ç”Ÿæˆå®Œæˆï¼")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}/")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()
