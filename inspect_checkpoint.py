#!/usr/bin/env python3
"""
æŸ¥çœ‹ PyTorch checkpoint (.pt) æ–‡ä»¶çš„å·¥å…·è„šæœ¬
ç”¨æ³•: python inspect_checkpoint.py <checkpoint_path>
"""

import torch
import sys
import os

def inspect_checkpoint(ckpt_path):
    """æ£€æŸ¥ checkpoint æ–‡ä»¶å†…å®¹"""

    if not os.path.exists(ckpt_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
        return

    # è¯»å–checkpoint
    print(f"ğŸ“‚ è¯»å–: {ckpt_path}\n")
    checkpoint = torch.load(ckpt_path, map_location='cpu')

    print("=" * 80)
    print("Checkpoint å†…å®¹æ¦‚è§ˆ")
    print("=" * 80)

    # æŸ¥çœ‹checkpointåŒ…å«çš„key
    print("\nã€Checkpoint Keysã€‘")
    for key in checkpoint.keys():
        if key == 'model_state_dict':
            print(f"  - {key} (æ¨¡å‹å‚æ•°)")
        elif key == 'optimizer_state_dict':
            print(f"  - {key} (ä¼˜åŒ–å™¨çŠ¶æ€)")
        elif key == 'args':
            print(f"  - {key} (è®­ç»ƒå‚æ•°é…ç½®)")
        else:
            print(f"  - {key}")

    # æŸ¥çœ‹è®­ç»ƒå‚æ•° (å¦‚æœä¿å­˜äº†)
    if 'args' in checkpoint:
        print("\n" + "=" * 80)
        print("è®­ç»ƒå‚æ•°é…ç½® (args)")
        print("=" * 80)
        args = checkpoint['args']

        # æŒ‰ç±»åˆ«åˆ†ç»„æ˜¾ç¤º
        print("\nã€æ¨¡å‹æ¶æ„ã€‘")
        for key in ['n_layers', 'd_model', 'd_inner', 'n_head', 'conv_kernel_size', 'in_channel']:
            if key in args:
                print(f"  {key}: {args[key]}")

        print("\nã€æ­£åˆ™åŒ–ã€‘")
        for key in ['dropout']:
            if key in args:
                print(f"  {key}: {args[key]}")
        # weight_decay å¯èƒ½ä¸åœ¨argsä¸­
        if 'weight_decay' in args:
            print(f"  weight_decay: {args['weight_decay']}")
        else:
            print(f"  weight_decay: N/A (æœªä¿å­˜æˆ–ä¸ºé»˜è®¤å€¼0)")

        print("\nã€è®­ç»ƒé…ç½®ã€‘")
        for key in ['batch_size', 'learning_rate', 'windows_per_sample', 'epoch']:
            if key in args:
                print(f"  {key}: {args[key]}")

        print("\nã€æ•°æ®é…ç½®ã€‘")
        for key in ['win_len', 'sample_rate']:
            if key in args:
                print(f"  {key}: {args[key]}")

        print("\nã€Conformer-v2 æ”¹è¿›ç‰¹æ€§ã€‘")
        for key in ['gradient_scale', 'use_llrd', 'llrd_front_scale', 'llrd_back_scale',
                    'llrd_output_scale', 'output_grad_scale', 'use_gated_residual', 'use_mlp_head']:
            if key in args:
                print(f"  {key}: {args[key]}")

        print("\nã€Conformer ç‰¹æ€§ã€‘")
        for key in ['use_relative_pos', 'use_macaron_ffn', 'use_sinusoidal_pos']:
            if key in args:
                print(f"  {key}: {args[key]}")

        print("\nã€åˆ†å¸ƒå¼è®­ç»ƒã€‘")
        for key in ['use_ddp', 'workers']:
            if key in args:
                print(f"  {key}: {args[key]}")

    # æŸ¥çœ‹è®­ç»ƒçŠ¶æ€
    print("\n" + "=" * 80)
    print("è®­ç»ƒçŠ¶æ€")
    print("=" * 80)
    if 'epoch' in checkpoint:
        print(f"  å½“å‰ Epoch: {checkpoint['epoch'] + 1}")  # +1å› ä¸ºä¿å­˜æ—¶æ˜¯ä»0å¼€å§‹
    if 'step' in checkpoint:
        print(f"  å½“å‰ Step: {checkpoint['step']}")
        # è®¡ç®—å®é™…epoch
        if 'args' in checkpoint and 'batch_size' in checkpoint['args']:
            # å‡è®¾æ¯ä¸ªepochæœ‰158ä¸ªbatch (508*20/64)
            iter_per_epoch = 158
            actual_epoch = checkpoint['step'] // iter_per_epoch
            print(f"  å®é™… Epoch: ~{actual_epoch}")
    if 'learning_rate' in checkpoint:
        print(f"  Learning Rate: {checkpoint['learning_rate']:.6f}")

    # æŸ¥çœ‹æ¨¡å‹å‚æ•°ç»Ÿè®¡
    if 'model_state_dict' in checkpoint:
        print("\n" + "=" * 80)
        print("æ¨¡å‹å‚æ•°ç»Ÿè®¡")
        print("=" * 80)
        state_dict = checkpoint['model_state_dict']

        total_params = sum(p.numel() for p in state_dict.values())
        print(f"  æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
        print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / (1024*1024):.2f} MB (float32)")

        print(f"\n  å‚æ•°å±‚æ€»æ•°: {len(state_dict)}")

        print("\n  ã€å‰10ä¸ªå‚æ•°å±‚ã€‘")
        for i, (name, param) in enumerate(list(state_dict.items())[:10]):
            print(f"    {name:50s} {str(tuple(param.shape)):30s} {param.numel():>10,} å‚æ•°")

        print("\n  ã€å10ä¸ªå‚æ•°å±‚ã€‘")
        for name, param in list(state_dict.items())[-10:]:
            print(f"    {name:50s} {str(tuple(param.shape)):30s} {param.numel():>10,} å‚æ•°")

        # ç»Ÿè®¡å„æ¨¡å—å‚æ•°é‡
        print("\n  ã€æ¨¡å—å‚æ•°åˆ†å¸ƒã€‘")
        module_params = {}
        for name, param in state_dict.items():
            module_name = name.split('.')[0]
            if module_name not in module_params:
                module_params[module_name] = 0
            module_params[module_name] += param.numel()

        for module, params in sorted(module_params.items(), key=lambda x: x[1], reverse=True):
            print(f"    {module:30s} {params:>12,} å‚æ•° ({params/total_params*100:>5.1f}%)")

    # æŸ¥çœ‹ä¼˜åŒ–å™¨çŠ¶æ€
    if 'optimizer_state_dict' in checkpoint:
        print("\n" + "=" * 80)
        print("ä¼˜åŒ–å™¨çŠ¶æ€")
        print("=" * 80)
        opt_dict = checkpoint['optimizer_state_dict']
        print(f"  å‚æ•°ç»„æ•°é‡: {len(opt_dict.get('param_groups', []))}")
        if 'param_groups' in opt_dict:
            for i, group in enumerate(opt_dict['param_groups']):
                print(f"\n  å‚æ•°ç»„ {i}:")
                if 'name' in group:
                    print(f"    åç§°: {group['name']}")
                print(f"    å­¦ä¹ ç‡: {group.get('lr', 'N/A')}")
                print(f"    å‚æ•°æ•°é‡: {len(group.get('params', []))}")

    print("\n" + "=" * 80)
    print("âœ“ æ£€æŸ¥å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python inspect_checkpoint.py <checkpoint_path>")
        print("\nç¤ºä¾‹:")
        print("  python inspect_checkpoint.py test_results/experiment/model_step_1000.pt")
        sys.exit(1)

    ckpt_path = sys.argv[1]
    inspect_checkpoint(ckpt_path)
